{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GTwDFnjHMk25"
   },
   "source": [
    "\n",
    "# DATA301 - Lab 0\n",
    "## Spark Tutorial (first part)\n",
    "\n",
    "In this tutorial, you will learn how to use [Apache Spark](https://spark.apache.org) in local mode on a Colab enviroment.\n",
    "\n",
    "Adapted with permission for use in DATA301. Additional credits to [Tiziano Piccardi](http://piccardi.me/) for his Spark Tutorial used in the Applied Data Analysis class at EPFL and [Michele Catasta](https://www.linkedin.com/in/pirroh/) from Stanford's CS246.\n",
    "\n",
    "*Note: do not change the existing asserts otherwise you will not know if your code works for our tests. You are welcome to add your own asserts in new cells.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "eXQzA01OS_yQ"
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AbYZoVVWOZA5"
   },
   "source": [
    "We need to install software on the remote Colab environment. These two lines install Java (required for the backend Spark server) and the PySpark library."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "dhzk3GE6S9RC",
    "outputId": "4d9fe9c0-3df0-48e4-865d-0f46b97ea8b9"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m281.4/281.4 MB\u001b[0m \u001b[31m3.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m199.7/199.7 KB\u001b[0m \u001b[31m18.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h  Building wheel for pyspark (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
     ]
    }
   ],
   "source": [
    "!apt-get install openjdk-8-jdk-headless -qq > /dev/null\n",
    "!pip install -q pyspark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "73zKfopwS8dg"
   },
   "source": [
    "Run the cell below to download data from google drive to the remote Colab environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "i3q9oLAZKf9J",
    "outputId": "4c8fbd19-e95d-44ac-8493-3669399907e7"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('Aircraft_Glossary.json.gz', <http.client.HTTPMessage at 0x7f929c1cd220>)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import urllib.request\n",
    "url = 'https://drive.google.com/uc?export=download&confirm=t&id=1L6pCQkldvdBoaEhRFzL0VnrggEFvqON4'\n",
    "filename = 'Bombing_Operations.json.gz'\n",
    "urllib.request.urlretrieve(url, filename)\n",
    "\n",
    "url = 'https://drive.google.com/uc?export=download&confirm=t&id=1L6pCQkldvdBoaEhRFzL0VnrggEFvqON4'\n",
    "filename = 'Aircraft_Glossary.json.gz'\n",
    "urllib.request.urlretrieve(url, filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wA49WWqmO5rR"
   },
   "source": [
    "If you executed the cells above correctly, you should be able to see the files *Bombing_Operations.json.gz* and *Aircraft_Glossary.json.gz* under the \"Files\" tab on the left panel."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bGpujOxDSusu"
   },
   "source": [
    "### Start the Spark context\n",
    "Let's start the Spark context on the remote environment. After executing the cell below, an in memory process that handles Spark API calls is running and worker processes are ready to execute RDD operations on the big data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADzUhsZQYuHH"
   },
   "outputs": [],
   "source": [
    "import pyspark\n",
    "from pyspark import SparkConf, SparkContext\n",
    "from pyspark.sql import *\n",
    "spark = SparkSession.builder.master(\"local[*]\").appName('SparkExample').config(\n",
    "    \"spark.executor.memory\", \"1g\").config(\n",
    "        \"spark.executorEnv.PYTHONHASHSEED\",\"0\").config(\"spark.ui.port\", \"4050\"\n",
    "        ).getOrCreate()\n",
    "sc = spark.sparkContext"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "-n20ixkgSagD"
   },
   "source": [
    "You can easily check the current version and get the link of the web interface. In the Spark UI, you can monitor the progress of your job and debug the performance bottlenecks (if your Colab is running with a **local runtime**)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 221
    },
    "id": "Fl4RHbqFSagE",
    "outputId": "75ff11c4-0a6b-4788-c364-e27fdb1d4f9d"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "            <div>\n",
       "                <p><b>SparkSession - in-memory</b></p>\n",
       "                \n",
       "        <div>\n",
       "            <p><b>SparkContext</b></p>\n",
       "\n",
       "            <p><a href=\"http://5a6482c2aea3:4050\">Spark UI</a></p>\n",
       "\n",
       "            <dl>\n",
       "              <dt>Version</dt>\n",
       "                <dd><code>v3.3.2</code></dd>\n",
       "              <dt>Master</dt>\n",
       "                <dd><code>local[*]</code></dd>\n",
       "              <dt>AppName</dt>\n",
       "                <dd><code>SparkExample</code></dd>\n",
       "            </dl>\n",
       "        </div>\n",
       "        \n",
       "            </div>\n",
       "        "
      ],
      "text/plain": [
       "<pyspark.sql.session.SparkSession at 0x7f927fce4070>"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vg6KpvpTWfnr"
   },
   "source": [
    "## Warm up exercises (complete the <FILL IN> parts)\n",
    "These exercises don't use a dataset. Instead they create simple RDD structures from small arrays.\n",
    "\n",
    "Let us first create a small array that we will manipulate. For realistic applications, data will most likely be loaded from a file or from a data stream. The builtin python command range creates a sequence from 1 to n. Since we generated this data instead of loading it in a distributed fashion, we need to create an RDD. See https://spark.apache.org/docs/latest/rdd-programming-guide.html#resilient-distributed-datasets-rdds for more details.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "Hi1E1dh5WZeI",
    "outputId": "846e7343-79a4-43a4-83ba-3c2d0b6d2741"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "499500\n",
      "499500\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# First lets create some simple data, say the integers from 1 to 1000. We use the python command range to do this. Note that in practice our data will most likely come from data files. \n",
    "A = range(1000)\n",
    "\n",
    "# Let us start with a simple task of computing the sum of the values in the array:\n",
    "print(sum(A))\n",
    "# Because A is in memory in our Google Colab python instance and sum is a built in python function\n",
    "# this computation is not distributed and instead runs sequentially.\n",
    "\n",
    "# Now let us distribute this data across all our processes using the sc.parallelize function.\n",
    "pA = sc.parallelize(A) \n",
    "\n",
    "# To sum our distributed array, we call the reduce function with a lambda function that adds two values. \n",
    "print(pA.reduce(lambda a,b: a+b))\n",
    "# Because the data structure pA was distributed across the available processes, the computation for reduction was also distributed.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "f9MYhJfjZqUA"
   },
   "source": [
    "###Problem 1 (5 points) \n",
    "Now that we have seen a simple example, write **sequential** code that implements the example from the lecture\n",
    "\n",
    "![image.png](data:image/png;base64,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)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3pmRbY0lZmUV"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "def sum_sqrts(A):\n",
    "    sum = 0\n",
    "    for item in A:\n",
    "      sum += math.sqrt(item)\n",
    "    return sum\n",
    "    raise NotImplementedError()\n",
    "\n",
    "assert sum_sqrts([1,4,9,16]) == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6NIq1WaHsHrG"
   },
   "outputs": [],
   "source": [
    "assert sum_sqrts([1,4,9,16]) == 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iqUig4Oian8m"
   },
   "source": [
    "### Problem 2 (5 points)\n",
    "Now write a *parallel* version of the code using Spark. You will need to use both map and reduce (see https://spark.apache.org/docs/latest/rdd-programming-guide.html#rdd-operations).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "26mfNm2Ua9NI"
   },
   "outputs": [],
   "source": [
    "#note: pA is assumed to be an RDD already, see test case\n",
    "def sum_sqrts_parallel(pA):\n",
    "    output = pA.map(lambda a: math.sqrt(a))\n",
    "    output = output.reduce(lambda a, b: a + b)\n",
    "    return output\n",
    "    raise NotImplementedError()\n",
    "\n",
    "assert sum_sqrts_parallel(sc.parallelize([1,4,9,16])) == 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "P20DQ5KOsHrG"
   },
   "outputs": [],
   "source": [
    "assert sum_sqrts_parallel(sc.parallelize([1,4,9,16])) == 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5mxZPnvsbg3G"
   },
   "source": [
    "### Problem 3 (15 points)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "6OrCF47SbjfR",
    "outputId": "fb3b4845-aaf9-4425-de5a-0192e1cd24bd"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0, 0, 1, 0, 1, 2, 0, 1, 2, 3, 0, 1, 2, 3, 4]\n",
      "[2, 4]\n",
      "5\n"
     ]
    }
   ],
   "source": [
    "# Let us quickly review some basic transformations available within Spark. \n",
    "\n",
    "# Lets create a small list of numbers to play with. \n",
    "nums = sc.parallelize([1,2,3,4,5]) \n",
    "\n",
    "# retain elements passing a predicate \n",
    "evens = nums.filter(lambda x: x % 2 == 0) \n",
    "\n",
    "# map each element to zero or more others \n",
    "x = nums.flatMap(lambda x: range(x))\n",
    "\n",
    "# retrieve RDD contents as a local collection \n",
    "print(x.collect())\n",
    "\n",
    "# return first 2 elements\n",
    "print(evens.take(2))\n",
    "\n",
    "# count number of elements \n",
    "print(nums.count())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YpCWXdSGcjYr"
   },
   "source": [
    "In the code box below, generate an array of 5 random numbers between 1 and 6 and parallelize it. Write a function that consumes an RDD of numeric values and produces an RDD with that many copies of each value. [note: your code should not collect the RDD into a local data structure]. Some test cases:\n",
    "```\n",
    "print(repeat_nums(sc.parallelize([1, 3, 4, 2, 1])).collect())\n",
    "> [1, 3, 3, 3, 4, 4, 4, 4, 2, 2, 1]\n",
    "\n",
    "print(repeat_nums(sc.parallelize([5, 2, 6, 3, 1])).collect())\n",
    "> [5, 5, 5, 5, 5, 2, 2, 6, 6, 6, 6, 6, 6, 3, 3, 3, 1]\n",
    "```\n",
    "Hint: In python we can repeat using the overloaded multiplier on lists\n",
    "\n",
    "```\n",
    ">>> [5] * 4\n",
    "[5, 5, 5, 5]\n",
    "```\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "r44oZI2Ach97"
   },
   "outputs": [],
   "source": [
    "def repeat_nums(nums):\n",
    "    output = nums.flatMap(lambda x: [x] * x)\n",
    "    return output\n",
    "    raise NotImplementedError()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "4OJcep05ov9D",
    "outputId": "72a99711-68ca-4c4b-d31b-f68b63232dad"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1, 3, 3, 3, 4, 4, 4, 4, 2, 2, 1]\n",
      "[5, 5, 5, 5, 5, 2, 2, 6, 6, 6, 6, 6, 6, 3, 3, 3, 1]\n"
     ]
    }
   ],
   "source": [
    "print(repeat_nums(sc.parallelize([1, 3, 4, 2, 1])).collect())\n",
    "print(repeat_nums(sc.parallelize([5, 2, 6, 3, 1])).collect())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rsubxYxbsHrH"
   },
   "outputs": [],
   "source": [
    "assert repeat_nums(sc.parallelize([1, 3, 4, 2, 1])).collect() == [1, 3, 3, 3, 4, 4, 4, 4, 2, 2, 1]\n",
    "assert repeat_nums(sc.parallelize([5, 2, 6, 3, 1])).collect() == [5, 5, 5, 5, 5, 2, 2, 6, 6, 6, 6, 6, 6, 3, 3, 3, 1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "gewv-lKMSagI"
   },
   "source": [
    "## Dataset exercises: The Vietnam War\n",
    "\n",
    "For the following exercises you will write parallel code in PySpark to extract some data from a small dataset of historical data from the Vietname War.\n",
    "\n",
    "**Pres. Johnson**: _What do you think about this Vietnam thing? I’d like to hear you talk a little bit._\n",
    "\n",
    "**Sen. Russell**: _Well, frankly, Mr. President, it’s the damn worse mess that I ever saw, and I don’t like to brag and I never have been right many times in my life, but I knew that we were going to get into this sort of mess when we went in there._\n",
    "\n",
    "May 27, 1964\n",
    "\n",
    "![banner](https://raw.githubusercontent.com/epfl-ada/2019/c17af0d3c73f11cb083717b7408fedd86245dc4d/Tutorials/04%20-%20Scaling%20Up/img/banner.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "skjUv84VSagJ"
   },
   "source": [
    "----\n",
    "\n",
    "The Vietnam War, also known as the Second Indochina War, and in Vietnam as the Resistance War Against America or simply the American War, was a conflict that occurred in Vietnam, Laos, and Cambodia from 1 November 1955 to the fall of Saigon on 30 April 1975. It was the second of the Indochina Wars and was officially fought between North Vietnam and the government of South Vietnam.\n",
    "\n",
    "**The dataset describes all the air force operations during the Vietnam War.**\n",
    "\n",
    "**Bombing_Operations** [Get the dataset here](https://drive.google.com/a/epfl.ch/file/d/1L6pCQkldvdBoaEhRFzL0VnrggEFvqON4/view?usp=sharing)\n",
    "\n",
    "- AirCraft: _Aircraft model (example: EC-47)_\n",
    "- ContryFlyingMission: _Country_\n",
    "- MissionDate: _Date of the mission_\n",
    "- OperationSupported: _Supported War operation_ (example: [Operation Rolling Thunder](https://en.wikipedia.org/wiki/Operation_Rolling_Thunder))\n",
    "- PeriodOfDay: _Day or night_\n",
    "- TakeoffLocation: _Take off airport_\n",
    "- TimeOnTarget\n",
    "- WeaponType\n",
    "- WeaponsLoadedWeight\n",
    "\n",
    "**Dataset Information:**\n",
    "\n",
    "THOR is a painstakingly cultivated database of historic aerial bombings from World War I through Vietnam. THOR has already proven useful in finding unexploded ordnance in Southeast Asia and improving Air Force combat tactics:\n",
    "https://www.kaggle.com/usaf/vietnam-war-bombing-operations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VSWoULeWSagJ"
   },
   "source": [
    "###*Load the dataset*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "XLyVPuLXSagK"
   },
   "outputs": [],
   "source": [
    "Bombing_Operations = spark.read.json(\"Bombing_Operations.json.gz\").rdd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2-Rt-_QSdraI"
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1jP5IBezSagQ"
   },
   "source": [
    "Get a sample with `take()`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "jrJpiG5ISagQ",
    "outputId": "cc30e36d-bfa5-4c52-b596-c3160a5c62fb"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(AirCraft='EC-47', ContryFlyingMission='UNITED STATES OF AMERICA', MissionDate='1971-06-05', OperationSupported=None, PeriodOfDay='D', TakeoffLocation='TAN SON NHUT', TargetCountry='CAMBODIA', TimeOnTarget=1005.0, WeaponType=None, WeaponsLoadedWeight=0),\n",
       " Row(AirCraft='EC-47', ContryFlyingMission='UNITED STATES OF AMERICA', MissionDate='1972-12-26', OperationSupported=None, PeriodOfDay='D', TakeoffLocation='NAKHON PHANOM', TargetCountry='SOUTH VIETNAM', TimeOnTarget=530.0, WeaponType=None, WeaponsLoadedWeight=0),\n",
       " Row(AirCraft='RF-4', ContryFlyingMission='UNITED STATES OF AMERICA', MissionDate='1973-07-28', OperationSupported=None, PeriodOfDay='D', TakeoffLocation='UDORN AB', TargetCountry='LAOS', TimeOnTarget=730.0, WeaponType=None, WeaponsLoadedWeight=0)]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bombing_Operations.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iu7n8KKjSagS"
   },
   "source": [
    "Note that each RDD entry is a Row object. See https://spark.apache.org/docs/latest/api/python/reference/api/pyspark.sql.Row.html for a description on how to access columns using the python properties. Here is a simple example:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "id": "nwYZrwanSagT",
    "outputId": "7d21da57-4958-4d2b-8a4d-ea8ce2a7dfcc"
   },
   "outputs": [
    {
     "data": {
      "application/vnd.google.colaboratory.intrinsic+json": {
       "type": "string"
      },
      "text/plain": [
       "'EC-47'"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Bombing_Operations.first().AirCraft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hkKWEEcLSagW"
   },
   "source": [
    "### Problem 4 (15 points): How many missions did the UNITED STATES OF AMERICA fly?\n",
    "\n",
    "In each of these problems you will need to use RDD transformations and actions. You do not need to write full python functions, a lambda expression in each of the <FILL IN> blanks is sufficient. See https://spark.apache.org/docs/latest/rdd-programming-guide.html#transformations for a list of available RDD transformations and actions and what they expect.\n",
    "\n",
    "From the Bombing_Operations RDD count how many rows have ContryFlyingMission with the UNITED STATES OF AMERICA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "NZR5_BYhNrbF",
    "outputId": "6e25acb8-8b9b-4976-f7d8-6ff5ff6d111b"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3708997\n"
     ]
    }
   ],
   "source": [
    "rdd_filtered = Bombing_Operations.filter(\n",
    "    lambda x: x.ContryFlyingMission==\"UNITED STATES OF AMERICA\"\n",
    "    )\n",
    "print(rdd_filtered.count())\n",
    "# should output 3708997"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5eBJgkg1sHrI"
   },
   "outputs": [],
   "source": [
    "assert rdd_filtered.count() == 3708997"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "LLan66cXSagf"
   },
   "source": [
    "### Problem 5 (15 points): Show the unique countries targetted in missions on 1972-12-26"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "BXNKt5PHSagf",
    "outputId": "7764e3db-7a8f-498e-c809-c3fd0cad333d"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['SOUTH VIETNAM', 'NORTH VIETNAM', 'LAOS', 'CAMBODIA', 'THAILAND']\n"
     ]
    }
   ],
   "source": [
    "dec26 = Bombing_Operations.filter(\n",
    "    lambda x: x.MissionDate=='1972-12-26'\n",
    "    )\n",
    "target_countries = dec26.map(\n",
    "    lambda x: x.TargetCountry\n",
    "    )\n",
    "print(target_countries.distinct().collect())\n",
    "# should output ['SOUTH VIETNAM', 'NORTH VIETNAM', 'LAOS', 'CAMBODIA', 'THAILAND']\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "id": "7KbwXGMmsHrI"
   },
   "outputs": [],
   "source": [
    "assert target_countries.distinct().collect() == ['SOUTH VIETNAM', 'NORTH VIETNAM', 'LAOS', 'CAMBODIA', 'THAILAND']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "uEwBspLySagp"
   },
   "source": [
    "### Problem 6 (15 points): Who bombed this location?\n",
    "\n",
    "\n",
    "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/epfl-ada/2019/c17af0d3c73f11cb083717b7408fedd86245dc4d/Tutorials/04%20-%20Scaling%20Up/img/Hanoi_POL1966.jpg\">\n",
    "\n",
    "This picture is the Hanoi POL facility (North Vietnam) burning after it was attacked by the U.S. Air Force on 29 June 1966 in the context of the Rolling Thunder operation. \n",
    "\n",
    "We are interested in discovering what was the most common take-off location during that day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "uq6jqsgBSagq",
    "outputId": "4025ecf6-d6cb-42b7-c065-fb0a5c55cae4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('TAKHLI', 56), ('DANANG', 35), ('CONSTELLATION', 87), ('UBON AB', 44), ('UDORN AB', 44), ('KORAT', 55), ('RANGER', 35), ('HANCOCK (CVA-19)', 10), ('TAN SON NHUT', 26), ('CUBI PT', 1), ('CAM RANH BAY', 2)]\n"
     ]
    }
   ],
   "source": [
    "jun_29_operations = Bombing_Operations.filter(\n",
    "    lambda x: x.MissionDate=='1966-06-29'\n",
    "    )\n",
    "full_filter = jun_29_operations.filter(\n",
    "    lambda x: x.TargetCountry=='NORTH VIETNAM'\n",
    "    )\n",
    "by_takeofflocation_key_values = full_filter.map(\n",
    "    lambda x: (x.TakeoffLocation, 1)\n",
    "    )\n",
    "by_takeofflocation = by_takeofflocation_key_values.reduceByKey(\n",
    "    lambda x, y: x + y\n",
    "    )\n",
    "print(by_takeofflocation.collect())\n",
    "\n",
    "# should produce a (key, value) list of the most common take-off locations with ('CONSTELLATION', 87) as the most common"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "id": "jAXgnX73sHrI"
   },
   "outputs": [],
   "source": [
    "assert by_takeofflocation.collect() == [('TAKHLI', 56), ('DANANG', 35), ('CONSTELLATION', 87), ('UBON AB', 44), ('UDORN AB', 44), ('KORAT', 55), ('RANGER', 35), ('HANCOCK (CVA-19)', 10), ('TAN SON NHUT', 26), ('CUBI PT', 1), ('CAM RANH BAY', 2)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cE8asfBPSahD"
   },
   "source": [
    "<img style=\"float: right;\" src=\"https://raw.githubusercontent.com/epfl-ada/2019/c17af0d3c73f11cb083717b7408fedd86245dc4d/Tutorials/04%20-%20Scaling%20Up/img/USS_Constellation.jpg\">\n",
    "\n",
    "\n",
    "That day the most common take-off location was the ship USS Constellation (CV-64). We cannot univocally identify one take off location, but we can reduce the possible candidates.\n",
    "\n",
    "_USS Constellation (CV-64), a Kitty Hawk-class supercarrier, was the third ship of the United States Navy to be named in honor of the \"new constellation of stars\" on the flag of the United States. One of the fastest ships in the Navy, as proven by her victory during a battlegroup race held in 1985, she was nicknamed \"Connie\" by her crew and officially as \"America's Flagship\"._"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aaiVcdF8SahD"
   },
   "source": [
    "### Problem 7 (20 points): What is the most used aircraft during the Vietnam war (number of missions)?\n",
    "\n",
    "In this question you must write your own code \"from scratch\". Hint: if you follow a similar process to problem 6 you can then use the takeOrdered action to get the most used aircraft. To get descending order you'll need to use a custom key expression to order by the negative count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "id": "G8m8yeW6l06q"
   },
   "outputs": [],
   "source": [
    "aircraft_key_values = Bombing_Operations.map(\n",
    "    lambda x: (x.AirCraft, 1)\n",
    "    )\n",
    "all_aircraft = aircraft_key_values.reduceByKey(\n",
    "    lambda x, y: x + y\n",
    "    )\n",
    "most_used = all_aircraft.takeOrdered(1, key = lambda x: -x[1])\n",
    "\n",
    "# should compute [('F-4', 909362)] as the result and assign it to the most_used variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "id": "K2MSA57ZsHrJ"
   },
   "outputs": [],
   "source": [
    "assert most_used == [('F-4', 909362)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XKloS3DauZ6h"
   },
   "source": [
    "## Submission\n",
    "You should go to File->Download->Download .ipynb and then upload that file to the last question in the Learn assessment."
   ]
  }
 ],
 "metadata": {
  "colab": {
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
